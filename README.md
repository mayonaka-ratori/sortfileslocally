<div align="center">
  <img src="https://via.placeholder.com/150/09090b/4f46e5?text=LCP" alt="Local Curator Prime Logo" width="100"/>
  <h1>Local Curator Prime</h1>
  <p><strong>AI-Powered Local Media Manager & Semantic Search</strong></p>
  <p><em>AIé§†å‹•ã®ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¡ãƒ‡ã‚£ã‚¢ç®¡ç†ï¼†ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ </em></p>
</div>

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.10%2B-blue" alt="Python Version">
  <img src="https://img.shields.io/badge/Next.js-16-black" alt="Next.js">
  <img src="https://img.shields.io/badge/FastAPI-0.109-009688" alt="FastAPI">
  <img src="https://img.shields.io/badge/PyTorch-2.0%2B-ee4c2c" alt="PyTorch">
  <img src="https://img.shields.io/badge/Tailwind_CSS-4-38bdf8" alt="Tailwind CSS">
</p>

---

## ğŸŒŸ Overview / æ¦‚è¦

**Local Curator Prime** is an advanced, offline-first media management tool designed to intelligently categorize, tag, and search through tens of thousands of local images and videos using cutting-edge Vision-Language Models (VLMs) and Semantic Search.

**Local Curator Prime** ã¯ã€æœ€å…ˆç«¯ã®è¦–è¦šãƒ»è¨€èªãƒ¢ãƒ‡ãƒ«(VLM)ã¨ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã‚’ç”¨ã„ã¦ã€æ•°ä¸‡æšä»¥ä¸Šã®ãƒ­ãƒ¼ã‚«ãƒ«ç”»åƒã‚„å‹•ç”»ã‚’è‡ªå‹•ã§åˆ†é¡ãƒ»ã‚¿ã‚°ä»˜ã‘ã—ã€è‡ªç„¶è¨€èªã§æ¤œç´¢ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ã€å®Œå…¨ã‚ªãƒ•ãƒ©ã‚¤ãƒ³å‹•ä½œã®é«˜åº¦ãªãƒ¡ãƒ‡ã‚£ã‚¢ç®¡ç†ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚

> **Note**: This repository is a portfolio showcase. The core AI models are downloaded locally upon first execution.
> 
> **æ³¨æ„**: ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã¯ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªç”¨ã®å…¬é–‹ãƒ¢ãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’å«ã¿ã¾ã™ã€‚ã‚³ã‚¢ã¨ãªã‚‹AIãƒ¢ãƒ‡ãƒ«ã¯åˆå›å®Ÿè¡Œæ™‚ã«ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã¸ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã™ã€‚

![Dashboard Mockup](docs/mockup_hero.png)
*(Mockup image of the dashboard interface / ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰UIã®ãƒ¢ãƒƒã‚¯ã‚¢ãƒƒãƒ—ç”»åƒ)*

---

## âœ¨ Key Features / ä¸»ãªæ©Ÿèƒ½

### ğŸ” 1. Semantic Search (ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢)
Instead of matching file names or manual tags, search your local media using natural language descriptions (e.g., *"a girl holding a red umbrella under the rain"*).
ãƒ•ã‚¡ã‚¤ãƒ«åã‚„æ‰‹å‹•ã‚¿ã‚°ã«é ¼ã‚‰ãšã€ã€Œé›¨ã®ä¸­ã§èµ¤ã„å‚˜ã‚’æŒã¤å¥³ã®å­ã€ã®ã‚ˆã†ãª**è‡ªç„¶è¨€èªã®è¨˜è¿°ã§ç”»åƒãƒ»å‹•ç”»ã‚’æ¤œç´¢**ã§ãã¾ã™ï¼ˆCLIP & FAISSé€£æºï¼‰ã€‚

### ğŸ¤– 2. Automated AI Tagging (è‡ªå‹•AIã‚¿ã‚°ä»˜ã‘)
Automatically extracts characters, series, and general tags from images using specialized models (e.g., DeepDanbooru/WD14), organizing your pristine collection instantly.
å°‚ç”¨ã®ç”»åƒèªè­˜ãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ã¦ã€ç”»åƒã‹ã‚‰**ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼åã€ä½œå“åã€ä¸€èˆ¬ã‚¿ã‚°ã‚’è‡ªå‹•æŠ½å‡º**ã—ã€æ‰‹å…ƒã®å¤§è¦æ¨¡ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç¬æ™‚ã«æ•´ç†ã—ã¾ã™ã€‚

### ğŸ¬ 3. Video Understanding (å‹•ç”»ãƒ»éŸ³å£°ç†è§£)
Fully supports video processing. Extracts keyframes for VLM description (Moondream2) and transcribes audio directly using local Whisper to enable deep scene search.
å‹•ç”»å‡¦ç†ã«å®Œå…¨å¯¾å¿œã€‚ã‚­ãƒ¼ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æŠ½å‡ºã—ã¦VLM(Moondream2)ã§ã‚·ãƒ¼ãƒ³å†…å®¹ã‚’èª¬æ˜ã•ã›ã€ãƒ­ãƒ¼ã‚«ãƒ«Whisperã§**éŸ³å£°ã‚’è‡ªå‹•æ–‡å­—èµ·ã“ã—**ã™ã‚‹ã“ã¨ã§ã€ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç‰¹å®šã®ã‚·ãƒ¼ãƒ³ã‚’ç¬æ™‚ã«æ¤œç´¢å¯èƒ½ã§ã™ã€‚

### ğŸ’¬ 4. Chat with Media (ç”»åƒãƒ»å‹•ç”»ã¨ã®å¯¾è©±)
Open any image in the gallery and chat directly with it using a Vision-Language Model. Ask about specific details, translate text in the image, or get creative prompts.
ã‚®ãƒ£ãƒ©ãƒªãƒ¼å†…ã®ç”»åƒã‚’å¯¾è±¡ã«ã€è¦–è¦šè¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆVLMï¼‰ã¨**ç›´æ¥ãƒãƒ£ãƒƒãƒˆ**ãŒå¯èƒ½ã€‚ç”»åƒå†…ã®è©³ç´°ã‚’å°‹ã­ãŸã‚Šã€æ–‡å­—ã‚’ç¿»è¨³ã—ãŸã‚Šã§ãã¾ã™ã€‚

### âš¡ 5. Offline & Privacy-First (å®Œå…¨ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ»ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼å„ªå…ˆ)
All heavy AI processing runs locally completely offline. No data is sent to external APIs (OpenAI, etc.), protecting your private collection. 
ã™ã¹ã¦ã®AIå‡¦ç†ï¼ˆæ¨è«–ã€ç‰¹å¾´é‡æŠ½å‡ºï¼‰ã¯ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§100%å®Œçµã€‚å¤–éƒ¨ã®APIã¸ãƒ‡ãƒ¼ã‚¿ã‚’é€ä¿¡ã™ã‚‹ã“ã¨ã¯ãªãã€å®‰å…¨ã«ç®¡ç†ã§ãã¾ã™ã€‚

---

## ğŸ— Architecture / ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ§‹æˆ

The system utilizes a modern web stack backed by a powerful local Python/PyTorch inference engine.
ãƒ¢ãƒ€ãƒ³ãªWebã‚¹ã‚¿ãƒƒã‚¯ã¨ã€å¼·åŠ›ãªãƒ­ãƒ¼ã‚«ãƒ«PyTorchæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã‚’çµ„ã¿åˆã‚ã›ã¦ã„ã¾ã™ã€‚

```mermaid
graph TD
    subgraph Frontend [Web Client - Next.js/React]
        UI[Gallery UI & Chat Panel]
        Sidebar[Search & Filters]
        ScanUI[Background Scan Monitor]
    end

    subgraph Backend [API Server - FastAPI]
        API[REST API Routes]
        DBManager[Database Manager]
    end

    subgraph CoreEngine [AI Inference Pipeline]
        Router[Task Router]
        CLIP[CLIP Encoder]
        VLM[Moondream2 / LLaVA]
        Faces[InsightFace / DeepDanbooru]
        Whisper[faster-whisper]
    end

    subgraph Storage [Local Data]
        SQLite[(SQLite DB)]
        FAISS[(FAISS Vector Index)]
        Files[Local Filesystem]
    end

    UI <-->|JSON/HTTP| API
    Sidebar <-->|Search Queries| API
    ScanUI <-->|Status Polling| API

    API <--> DBManager
    DBManager <--> SQLite
    DBManager <--> FAISS

    API <-->|Image/Video Paths| Router
    Router --> CLIP
    Router --> VLM
    Router --> Faces
    Router --> Whisper

    Router -->|Extracted Vectors & Tags| DBManager
    Router -.->|Read| Files
```

---

## ğŸ›  Tech Stack / æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯

### Frontend (User Interface)
- **Next.js 16** (React 19) - App Router
- **Tailwind CSS 4** - Styling & Dark Mode Aesthetics
- **Lucide React** - Iconography

### Backend (Server & API)
- **FastAPI** - High-performance Python web framework
- **Uvicorn** - ASGI server

### AI & Data Engine
- **PyTorch** - Core Deep Learning Framework
- **Transformers / OpenCLIP (Hugging Face)** - CLIP & Text processing
- **FAISS** - Ultra-fast vector similarity search for semantic queries
- **SQLite3** - Relational metadata storage
- **faster-whisper** - Optimized audio transcription
- **InsightFace / Decord** - Face detection & hardware-accelerated video decoding

---

## ğŸš€ Getting Started / ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ‰‹é †

### Prerequisites / å‰ææ¡ä»¶
- **OS**: Windows 10/11, Linux (Ubuntu recommended)
- **Python**: 3.10 or 3.11
- **Node.js**: v18+
- **GPU**: NVIDIA GPU with at least 8GB VRAM (CUDA 11.8+ installed recommended for performance). CPU fallback is supported but extremely slow.
*(VRAM 8GBä»¥ä¸Šã®NVIDIA GPUã‚’å¼·ãæ¨å¥¨ã—ã¾ã™ã€‚)*

### 1. Backend Setup (ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã®èµ·å‹•)

```bash
# Clone the repository
git clone https://github.com/yourusername/local-curator-prime.git
cd local-curator-prime

# Create virtual environment / ä»®æƒ³ç’°å¢ƒã®ä½œæˆ
python -m venv venv
# Windows: venv\Scripts\activate
# Linux/Mac: source venv/bin/activate

# Install dependencies (ensure PyTorch matches your CUDA version first)
# ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆå…ˆã«PyTorchã®CUDAãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’åˆã‚ã›ã¦ãã ã•ã„ï¼‰
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install -r requirements.txt

# Start the FastAPI server (downloads models on first run - approx. 6GB)
# FastAPIã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•ï¼ˆåˆå›ã¯ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒè¡Œã‚ã‚Œã¾ã™ - ç´„6GBï¼‰
cd server
python main.py
```

### 2. Frontend Setup (ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã®èµ·å‹•)

Open a new terminal. / æ–°ã—ã„ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã‚’é–‹ãã¾ã™ã€‚

```bash
cd local-curator-prime/web

# Install dependencies
npm install

# Start the development server
npm run dev
```

Visit `http://localhost:3000` in your browser.
ãƒ–ãƒ©ã‚¦ã‚¶ã§ `http://localhost:3000` ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ãã ã•ã„ã€‚

---

## ğŸ“¸ Screenshots / ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆ

*Screenshots feature mock data for portfolio presentation purposes.*

### Gallery View & Semantic Search
![Gallery Search Mockup](docs/mockup_search.png)
*Search for complex concepts and see exact text match snippets from video audio/frames.*

### Video Analysis & Chat
![Chat Panel Mockup](docs/mockup_chat.png)
*Chat directly with the local vision language model about any media file.*

---
*Built with â¤ï¸ for organizing massive digital collections.*

